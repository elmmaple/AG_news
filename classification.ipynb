{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Color\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to wandb\n",
    "wandb.login()\n",
    "# wandb config\n",
    "WANDB_CONFIG = {\n",
    "    'competition': 'AG News Classification Dataset', \n",
    "    '_wandb_kernel': 'neuracort'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Path\n",
    "TRAIN_FILE_PATH = 'data/train.csv'\n",
    "TEST_FILE_PATH = 'data/test.csv'\n",
    "#Load Data\n",
    "data = pd.read_csv(TRAIN_FILE_PATH)\n",
    "testdata = pd.read_csv(TEST_FILE_PATH)\n",
    "#Set Column Names \n",
    "data.columns = ['ClassIndex', 'Title', 'Description']\n",
    "testdata.columns = ['ClassIndex', 'Title', 'Description']\n",
    "#Combine Title and Description\n",
    "#在進行自然語言處理的文本分類任務時，通常將標題（Title）和描述（Description）合併在一起作為模型的輸入文本是一個常見的做法，這樣做的好處如下：\n",
    "# 更豐富的信息：將標題和描述合併在一起，可以提供更多的文本信息給模型，這有助於模型更好地理解文本的內容。\n",
    "# 更好的模型效果：通常情況下，使用合併後的文本作為輸入會比單獨使用標題或描述效果更好，因為模型可以從更長的文本序列中學習到更多的特徵和上下文信息。\n",
    "# 減少維度：合併標題和描述後，文本序列的長度相對較短，這樣可以減少模型的輸入維度，提高訓練效率。\n",
    "# 更好的泛化性能：合併文本可以幫助模型學習到更通用的特徵，使得模型在測試集上更好地泛化，而不會過度依賴某個特定部分的文本。\n",
    "# 總的來說，將標題和描述合併在一起可以幫助模型更好地理解文本，提高模型的效果和泛化性能。當然，根據具體任務和數據集的特點，也可以嘗試單獨使用標題或描述作為輸入進行實驗，看哪種方式效果更好。\n",
    "X_train = data['Title'] + \" \" + data['Description'] # Combine title and description (better accuracy than using them as separate features)\n",
    "y_train = data['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n",
    "\n",
    "x_test = testdata['Title'] + \" \" + testdata['Description'] # Combine title and description (better accuracy than using them as separate features)\n",
    "y_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n",
    "\n",
    "#Max Length of sentences in Train Dataset\n",
    "maxlen = X_train.map(lambda x: len(x.split())).max()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Value counts to determine class balance\n",
    "data.ClassIndex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata.ClassIndex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 # arbitrarily chosen\n",
    "embed_size = 20 # arbitrarily chosen\n",
    "\n",
    "# Create and Fit tokenizer\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(X_train.values)\n",
    "\n",
    "# Tokenize data\n",
    "X_train = tok.texts_to_sequences(X_train)\n",
    "x_test = tok.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad data\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='ag-news', config= WANDB_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True))) \n",
    "# Define the layers and their corresponding sizes\n",
    "layers = [1024, 512, 256, 128, 64]\n",
    "# Define the dropout rate\n",
    "dropout_rate = 0.2\n",
    "for layer_size in layers:\n",
    "    model.add(Bidirectional(LSTM(layer_size, return_sequences=True)))\n",
    "    model.add(Dense(layer_size)) #softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n",
    "    model.add(Dropout(dropout_rate))\n",
    "model.add(GlobalMaxPooling1D()) #Pooling Layer decreases sensitivity to features, thereby creating more generalised data for better test results.\n",
    "model.add(Dense(4, activation='softmax')) #softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping：\n",
    "\n",
    "# monitor: 監控的指標，這裡設置為 'val_accuracy'，表示監控驗證集的準確度。\n",
    "# min_delta: 當驗證指標的變化小於 min_delta 時，將被認為沒有改善。\n",
    "# patience: 在指標沒有改善的情況下，經過幾個 epoch 後停止訓練。\n",
    "# verbose: 控制是否輸出訓練過程中的信息，1 表示輸出。\n",
    "# ModelCheckpoint：\n",
    "\n",
    "# filepath: 模型的權重（weights）保存的文件路徑。\n",
    "# monitor: 監控的指標，這裡設置為 'val_accuracy'，表示監控驗證集的準確度。\n",
    "# mode: 設置為 'max' 表示監控指標的最大值，當指標達到最大值時保存模型權重。\n",
    "# save_best_only: 只保存在指標上表現最好的模型權重。\n",
    "# save_weights_only: 設置為 True 僅保存模型的權重而不保存模型結構。\n",
    "# verbose: 控制是否輸出保存模型權重的信息，1 表示輸出。\n",
    "# WandbCallback：\n",
    "\n",
    "# WandbCallback 是用於將模型訓練過程中的資訊記錄到 Weights & Biases 平台的回調函式。\n",
    "# Wandb 是一個用於跟蹤機器學習實驗的工具，可以記錄模型的準確度、損失函數、學習率等訓練過程中的指標，方便進行實驗結果的比較和分析\n",
    "callbacks = [\n",
    "    EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n",
    "        monitor='val_accuracy',\n",
    "        min_delta=1e-4,\n",
    "        patience=4,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='weights.h5',\n",
    "        monitor='val_accuracy', \n",
    "        mode='max', \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    WandbCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile and Fit Model\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=256, \n",
    "          validation_data=(x_test, y_test), \n",
    "          epochs=2, \n",
    "          callbacks=callbacks)\n",
    "\n",
    "# Close W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.h5')\n",
    "model.save('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelDemo(news_text):\n",
    "\n",
    "  #News Labels\n",
    "  labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n",
    "\n",
    "  test_seq = pad_sequences(tok.texts_to_sequences(news_text), maxlen=maxlen)\n",
    "\n",
    "  test_preds = [labels[np.argmax(i)] for i in model.predict(test_seq)]\n",
    "\n",
    "  for news, label in zip(news_text, test_preds):\n",
    "      # print('{} - {}'.format(news, label))\n",
    "      print('{} - {}'.format(colored(news, 'yellow'), colored(label, 'blue')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#輸入了新的文本數據並預測其類別\n",
    "modelDemo(['New evidence of virus risks from wildlife trade'])\n",
    "modelDemo(['Coronavirus: Bank pumps £100bn into UK economy to aid recovery'])\n",
    "modelDemo(['Trump\\'s bid to end Obama-era immigration policy ruled unlawful'])\n",
    "modelDemo(['David Luiz’s future with Arsenal to be decided this week'])\n",
    "modelDemo(['Indian Economic budget supports the underprivileged sections of society'])\n",
    "labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n",
    "preds = [np.argmax(i) for i in model.predict(x_test)]\n",
    "cm  = confusion_matrix(y_test, preds)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "plt.xticks(range(4), labels, fontsize=12)\n",
    "plt.yticks(range(4), labels, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall of the model is {:.2f}\".format(recall_score(y_test, preds, average='micro')))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision_score(y_test, preds, average='micro')))\n",
    "print(\"Accuracy of the model is {:.2f}\".format(accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# 對測試集的預測結果取最大概率對應的類別標籤\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "# 計算 f1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 score:{f1:.2f}\")\n",
    "f1 = f1_score(y_test, y_pred, average='micro') \n",
    "print(f\"F1 score:{f1:.2f}\")\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"F1 score:{f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
